<!-- <details>
<summary><a href="X">X</a></summary>

X

</details> -->




<details>
<summary><a href="https://arxiv.org/pdf/2206.15475">Causal Machine Learning: A Survey and Open Problems</a></summary>

Content hidden until toggled.

</details>
<details>
<summary><a href"https://arxiv.org/pdf/2307.05704">A Causal Ordering Prior for
Unsupervised Representation Learning</a></summary>

Content hidden until toggled.

</details>
<details>
<summary><a href"https://arxiv.org/pdf/2406.13371">Identifiable Causal Representation Learning</a></summary>

Content hidden until toggled.

</details>
<details>
<summary><a href"https://arxiv.org/pdf/2102.11107">Towards Causal Representation Learning</a></summary>

Content hidden until toggled.

</details>
<details>
<summary><a href"https://www.ucl.ac.uk/~ucgtrbd/papers/causality.pdf">Causality Ricardo Silva</a></summary>

Content hidden until toggled.

</details>
<details>
<summary><a href"https://web.cs.ucla.edu/~kaoru/3-layer-causal-hierarchy.pdf">The Three Layer Causal Hierarchy</a></summary>

Some thoughts: We use association to infer some probability distribution P(y|x). We then test this hypothesis against our environment. We take actions to learn a P(y|do(x)) distribution of the world. If P(y|x) != P(y|do(x)), then we pay attention to it. We adjust our synapses/parameters such that our P(y|x) changes to match the causal distribution.

Note: There is a fine distinction between intervention and counterfactual: counterfactuals are not just operating within the Markovian current state â€” they require disentangling the state into its causal components. So while you might intuitively say that counterfactuals are just intervention after taking an action, there is a slight difference. With counterfactuals, your world model is directly abstracting causality of prior events (think System 2) rather than just updating the the world model (System 1).

</details>
<details>
<summary><a href="https://arxiv.org/pdf/1910.01075">Learning Neural Causal Models from Unknown Interventions</a></summary>

Content hidden until toggled.

</details>
<details>
<summary><a href="https://arxiv.org/pdf/1901.10912">A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms</a></summary>

Content hidden until toggled.

</details>
<details>
<summary><a href="https://arxiv.org/pdf/1907.02893">Invariant Risk Minimization</a></summary>

Content hidden until toggled.

</details>
